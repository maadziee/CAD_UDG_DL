{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1663ae2e-88d4-41d7-a31b-28c1240f8d5a",
      "metadata": {
        "tags": [],
        "id": "1663ae2e-88d4-41d7-a31b-28c1240f8d5a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, models\n",
        "from torchvision.models.resnet import ResNet50_Weights\n",
        "\n",
        "\n",
        "from utils.dataset import SkinDataset\n",
        "from utils.utils import train, validate, test, load_data_file\n",
        "from utils.metric import MetricsMonitor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e14ab6e-615a-48bf-8709-55251f138bc0",
      "metadata": {
        "id": "7e14ab6e-615a-48bf-8709-55251f138bc0"
      },
      "source": [
        "## Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7351166-ac79-42b0-8e12-79c4b173142c",
      "metadata": {
        "tags": [],
        "id": "b7351166-ac79-42b0-8e12-79c4b173142c"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip images horizontally with 50% probability\n",
        "    transforms.RandomAffine(\n",
        "        degrees=15,                           # Random rotation within [-15, 15] degrees\n",
        "        translate=(0.1, 0.1),                 # Random shift by up to 10% in both x and y directions\n",
        "    ),\n",
        "    transforms.Resize((224, 224)),           # Resize to match ResNet input size\n",
        "    transforms.ToTensor(),                   # Convert image to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize with ImageNet stats\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c3349c3-cdad-4881-b5c5-8dfddcebb3a5",
      "metadata": {
        "id": "3c3349c3-cdad-4881-b5c5-8dfddcebb3a5"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7c240c1-40f8-4248-a383-2b3eade71e23",
      "metadata": {
        "tags": [],
        "id": "a7c240c1-40f8-4248-a383-2b3eade71e23"
      },
      "outputs": [],
      "source": [
        "CLASSES = ['nevus', 'others']\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "LR = 0.0005\n",
        "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a5b566c-0dc0-4617-a3a6-a17667754e22",
      "metadata": {
        "tags": [],
        "id": "4a5b566c-0dc0-4617-a3a6-a17667754e22"
      },
      "outputs": [],
      "source": [
        "## Load data paths and labels\n",
        "train_path, train_labels = load_data_file('datasets/train.txt')\n",
        "train_path, val_path, train_labels, val_labels = train_test_split(train_path, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n",
        "test_path, test_labels = load_data_file('datasets/val.txt')\n",
        "\n",
        "## Create datasets and dataloaders\n",
        "train_dataset = SkinDataset(train_path, train_labels, transform)\n",
        "val_dataset = SkinDataset(val_path, val_labels, transform)\n",
        "test_dataset = SkinDataset(test_path, test_labels, transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93041416",
      "metadata": {
        "id": "93041416",
        "outputId": "a6d42322-2f6d-4591-b5ca-a902d4c0bff2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================== Train dataset Info: ==================\n",
            " Dataset: 12156 samples\n",
            "Class distribution: {0: 6180, 1: 5976}\n",
            "\n",
            "================== Val dataset Info: ==================\n",
            " Dataset: 3039 samples\n",
            "Class distribution: {1: 1494, 0: 1545}\n",
            "\n",
            "================== Test dataset Info: ==================\n",
            " Dataset: 3796 samples\n",
            "Class distribution: {0: 1931, 1: 1865}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('================== Train dataset Info: ==================\\n', train_dataset)\n",
        "print('================== Val dataset Info: ==================\\n', val_dataset)\n",
        "print('================== Test dataset Info: ==================\\n', test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39ecbcaf-c933-4a25-947d-4c60d35d62ca",
      "metadata": {
        "id": "39ecbcaf-c933-4a25-947d-4c60d35d62ca"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62f5cfc8-0e48-4986-9ad5-72a10efdda18",
      "metadata": {
        "tags": [],
        "id": "62f5cfc8-0e48-4986-9ad5-72a10efdda18"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, len(CLASSES))\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e8adbce-010e-4a81-8c43-9b958e9538d2",
      "metadata": {
        "id": "2e8adbce-010e-4a81-8c43-9b958e9538d2"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e9c4299-8a3f-41cd-95e6-86df310cddea",
      "metadata": {
        "tags": [],
        "id": "5e9c4299-8a3f-41cd-95e6-86df310cddea"
      },
      "outputs": [],
      "source": [
        "# Loss and Optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# Monitors\n",
        "train_monitor = MetricsMonitor(metrics=[\"loss\", \"accuracy\"])\n",
        "val_monitor = MetricsMonitor(metrics=[\"loss\", \"accuracy\"], patience=5, mode=\"max\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c50415a4-b0b3-4f5b-a7f4-9423ebc93c17",
      "metadata": {
        "tags": [],
        "id": "c50415a4-b0b3-4f5b-a7f4-9423ebc93c17"
      },
      "outputs": [],
      "source": [
        "# Training Loop\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
        "    train(model, train_loader, criterion, optimizer, device, train_monitor)\n",
        "    validate(model, val_loader, criterion, device, val_monitor)\n",
        "    val_acc = val_monitor.compute_average(\"accuracy\")\n",
        "    if val_monitor.early_stopping_check(val_acc, model):\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb014470",
      "metadata": {
        "id": "bb014470"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb8d7363",
      "metadata": {
        "id": "cb8d7363"
      },
      "outputs": [],
      "source": [
        "test(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec2f7c2c",
      "metadata": {
        "id": "ec2f7c2c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11ac6feb",
      "metadata": {
        "id": "11ac6feb"
      },
      "outputs": [],
      "source": [
        "import nbformat\n",
        "\n",
        "# Load the notebook\n",
        "notebook_path = \"Binary.ipynb\"\n",
        "output_script_path = \"exp.py\"\n",
        "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
        "    notebook = nbformat.read(f, as_version=4)\n",
        "\n",
        "# Extract code cells only\n",
        "code_cells = [cell['source'] for cell in notebook.cells if cell.cell_type == 'code']\n",
        "code_cells = code_cells[:-1]  # Exclude the last cell\n",
        "\n",
        "# Save as a .py file\n",
        "with open(output_script_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"\\n\\n\".join(code_cells))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9914a67f",
      "metadata": {
        "id": "9914a67f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b11a5fa5",
      "metadata": {
        "id": "b11a5fa5",
        "outputId": "3d4beaf6-3ec4-4812-a28a-022a89f8cd6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting basic-image-eda\n",
            "  Downloading basic_image_eda-0.0.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Downloading basic_image_eda-0.0.3-py3-none-any.whl (9.8 kB)\n",
            "Installing collected packages: basic-image-eda\n",
            "Successfully installed basic-image-eda-0.0.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install basic-image-eda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a948cbc",
      "metadata": {
        "id": "9a948cbc",
        "outputId": "4efc6ed0-8e77-4db2-d132-efb1a7b662fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "found 18991 images.\n",
            "Using 16 threads. (max:16)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 18991/18991 [01:29<00:00, 213.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "*--------------------------------------------------------------------------------------*\n",
            "number of images                         |  18991\n",
            "\n",
            "dtype                                    |  uint8\n",
            "channels                                 |  [3]\n",
            "extensions                               |  ['jpg']\n",
            "\n",
            "min height                               |  450\n",
            "max height                               |  1024\n",
            "mean height                              |  761.2003580643462\n",
            "median height                            |  768\n",
            "\n",
            "min width                                |  576\n",
            "max width                                |  1024\n",
            "mean width                               |  855.1536517297667\n",
            "median width                             |  1024\n",
            "\n",
            "mean height/width ratio                  |  0.8901328510082651\n",
            "median height/width ratio                |  0.75\n",
            "recommended input size(by mean)          |  [760 856] (h x w, multiples of 8)\n",
            "recommended input size(by mean)          |  [768 848] (h x w, multiples of 16)\n",
            "recommended input size(by mean)          |  [768 864] (h x w, multiples of 32)\n",
            "\n",
            "channel mean(0~1)                        |  [0.6665315  0.52944165 0.5243559 ]\n",
            "channel std(0~1)                         |  [0.22581616 0.20484802 0.21641603]\n",
            "*--------------------------------------------------------------------------------------*\n",
            "eda ended in 00 hours 01 minutes 29 seconds\n",
            "No images found.\n"
          ]
        },
        {
          "ename": "SystemExit",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/miniconda3/envs/py11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "from basic_image_eda import BasicImageEDA\n",
        "\n",
        "if __name__ == \"__main__\":  # for multiprocessing\n",
        "    data_dir = \"./data\"\n",
        "    BasicImageEDA.explore('/root/madz/datasets/Binary')\n",
        "\n",
        "    # or\n",
        "\n",
        "    extensions = ['png', 'jpg', 'jpeg']\n",
        "    threads = 0\n",
        "    dimension_plot = True\n",
        "    channel_hist = True\n",
        "    nonzero = False\n",
        "    hw_division_factor = 1.0\n",
        "\n",
        "    BasicImageEDA.explore(data_dir, extensions, threads, dimension_plot, channel_hist, nonzero, hw_division_factor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35aed4d7",
      "metadata": {
        "id": "35aed4d7",
        "outputId": "c4c2731b-5868-4dd3-d401-18a8483ee3c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "os.cpu_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "615ec2c6",
      "metadata": {
        "id": "615ec2c6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}